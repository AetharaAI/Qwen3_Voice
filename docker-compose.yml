version: '3.8'

services:
  # --- VLLM-OMNI SERVICE (Voice-to-Voice) ---
  omni-service:
    build:
      context: ./vllm-omni
      dockerfile: Dockerfile
      args:
        CUDA_VERSION: 12.1.0
        PYTHON_VERSION: 3.10
    container_name: omni_voice_service
    restart: unless-stopped
    ports:
      - "8004:8000"
    expose:
      - "8000"
    environment:
      - HF_HOME=/mnt/aetherpro-extra1/hf
      - TRANSFORMERS_CACHE=/mnt/aetherpro-extra1/hf
      - HF_DATASETS_CACHE=/mnt/aetherpro-extra1/hf
      - CUDA_VISIBLE_DEVICES=1
      - VLLM_LOGGING_LEVEL=INFO
    volumes:
      - /mnt/aetherpro-extra1/hf:/mnt/aetherpro-extra1/hf
      - omni_model_cache:/root/.cache/vllm
    command: >
      python -m vllm.entrypoints.openai.api_server
      --model cyankiwi/Qwen3-Omni-30B-A3B-Instruct-AWQ-4bit
      --quantization awq
      --gpu-memory-utilization 0.65
      --max-model-len 32768
      --port 8000
      --host 0.0.0.0
      --omni
      --enable-chunked-prefill
      --tensor-parallel-size 1
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              device_ids: ['1']
              capabilities: [gpu]
    healthcheck:
      test: ["CMD", "python", "-c", "import urllib.request; urllib.request.urlopen('http://localhost:8000/health')"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 120s
    networks:
      - voice-gateway

  # --- VLLM-TTS SERVICE (Read-Aloud) ---
  tts-service:
    image: vllm/vllm-openai:latest
    container_name: tts_read_service
    restart: unless-stopped
    expose:
      - "8001"
    environment:
      - HF_HOME=/mnt/aetherpro-extra1/hf
      - TRANSFORMERS_CACHE=/mnt/aetherpro-extra1/hf
      - HF_DATASETS_CACHE=/mnt/aetherpro-extra1/hf
      - CUDA_VISIBLE_DEVICES=1
      - VLLM_LOGGING_LEVEL=INFO
    volumes:
      - /mnt/aetherpro-extra1/hf:/mnt/aetherpro-extra1/hf
      - tts_model_cache:/root/.cache/vllm
    command: >
      python -m vllm.entrypoints.openai.api_server
      --model Qwen/Qwen3-TTS-12Hz-0.6B-Base
      --gpu-memory-utilization 0.10
      --max-model-len 4096
      --port 8001
      --host 0.0.0.0
      --dtype float16
      --tensor-parallel-size 1
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              device_ids: ['1']
              capabilities: [gpu]
    healthcheck:
      test: ["CMD", "python", "-c", "import urllib.request; urllib.request.urlopen('http://localhost:8001/health')"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 60s
    networks:
      - voice-gateway

  # --- GATEWAY SERVICE (FastAPI Orchestration) ---
  gateway:
    build:
      context: ./gateway
      dockerfile: Dockerfile
    container_name: voice_gateway
    restart: unless-stopped
    ports:
      - "8003:8000"
    environment:
      - GATEWAY_PORT=8000
      - OMNI_SERVICE_URL=http://omni-service:8000
      - OMNI_WS_URL=ws://omni-service:8000
      - TTS_SERVICE_URL=http://tts-service:8001
      - GPU_DEVICE=1
      - AUDIO_SAMPLE_RATE=24000
      - AUDIO_FORMAT=pcm
      - OMNI_MODEL_NAME=cyankiwi/Qwen3-Omni-30B-A3B-Instruct-AWQ-4bit
      - TTS_MODEL_NAME=Qwen/Qwen3-TTS-12Hz-0.6B-Base
      - HF_HOME=/mnt/aetherpro-extra1/hf
      - PYTHONUNBUFFERED=1
    volumes:
      - /mnt/aetherpro-extra1/hf:/mnt/aetherpro-extra1/hf
    depends_on:
      omni-service:
        condition: service_healthy
      tts-service:
        condition: service_healthy
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              device_ids: ['1']
              capabilities: [gpu]
    networks:
      - voice-gateway
    healthcheck:
      test: ["CMD", "python", "-c", "import urllib.request; urllib.request.urlopen('http://localhost:8000/health')"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s

volumes:
  omni_model_cache:
    driver: local
  tts_model_cache:
    driver: local

networks:
  voice-gateway:
    name: voice-gateway
    driver: bridge